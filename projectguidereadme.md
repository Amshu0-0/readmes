# Lecture to Notes App - Complete Project Guide

This document explains every file in the project, what it does, and how each function works. Think of this as your study guide to understand the entire application.

---

## Table of Contents
1. [Project Overview](#project-overview)
2. [How the App Works (Big Picture)](#how-the-app-works-big-picture)
3. [File Structure](#file-structure)
4. [Backend Files (Server)](#backend-files-server)
5. [Frontend Files (Client)](#frontend-files-client)
6. [Configuration Files](#configuration-files)
7. [How Everything Connects](#how-everything-connects)

---

## Project Overview

**What is this app?**
This is a web application that records lectures and automatically converts them into organized notes. It's like having an AI teaching assistant that listens to a lecture and writes detailed notes for you.

**The Process:**
1. Student clicks "Start Recording"
2. Browser records audio from microphone
3. Audio is uploaded to our server
4. Google Speech-to-Text converts audio to text (transcript)
5. OpenAI organizes the transcript into structured notes
6. Student sees both the transcript and organized notes

**Technology Stack:**
- **Frontend**: React (user interface)
- **Backend**: Node.js + Express (server)
- **AI Services**: Google Speech-to-Text API + OpenAI GPT-3.5-Turbo
- **Styling**: Tailwind CSS (makes things look pretty)

---

## How the App Works (Big Picture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚
â”‚  (Student)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. Records audio using microphone
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                  â”‚
       â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AudioRecorder â”‚              â”‚  Our Server  â”‚
â”‚   Component     â”‚â”€â”€2. Uploadâ”€â”€â–¶â”‚  (Express)   â”‚
â”‚   (React)       â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
       â–²                                â”‚
       â”‚                                â”‚ 3. Send to Google
       â”‚                                â–¼
       â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                         â”‚   Google     â”‚
       â”‚                         â”‚ Speech-to-   â”‚
       â”‚                         â”‚    Text      â”‚
       â”‚                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
       â”‚                                â”‚ 4. Returns transcript
       â”‚                                â–¼
       â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                         â”‚  Our Server  â”‚
       â”‚                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
       â”‚                                â”‚ 5. Send to OpenAI
       â”‚                                â–¼
       â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                         â”‚    OpenAI    â”‚
       â”‚                         â”‚  GPT-3.5     â”‚
       â”‚                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
       â”‚                                â”‚ 6. Returns notes
       â”‚                                â–¼
       â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€7. Display notesâ”€â”€â”€â”€â”€â”€â”‚  Our Server  â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
lecturetonotesapp/
â”‚
â”œâ”€â”€ server/                  # Backend code (Node.js server)
â”‚   â”œâ”€â”€ index.js            # Main server file
â”‚   â”œâ”€â”€ .gitignore          # Tells git to ignore uploads folder
â”‚   â””â”€â”€ uploads/            # Temporary storage for audio files
â”‚
â”œâ”€â”€ client/                  # Frontend code (React app)
â”‚   â”œâ”€â”€ public/             # Static files
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx         # Main React component
â”‚   â”‚   â”œâ”€â”€ index.css       # Styling
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â””â”€â”€ AudioRecorder.jsx  # The main recording component
â”‚   â”œâ”€â”€ package.json        # Frontend dependencies
â”‚   â””â”€â”€ package-lock.json   # Exact versions of dependencies
â”‚
â”œâ”€â”€ package.json            # Backend dependencies
â”œâ”€â”€ package-lock.json       # Exact versions of dependencies
â””â”€â”€ .env.example            # Template for environment variables
```

---

## Backend Files (Server)

### `server/index.js` - The Brain of the Backend

**What it does:**
This file is the server that handles all the heavy lifting. It receives audio files, talks to Google and OpenAI, and sends results back to the frontend.

**Dependencies it uses:**
- `express` - Creates the web server
- `multer` - Handles file uploads
- `@google-cloud/speech` - Connects to Google Speech-to-Text
- `openai` - Connects to OpenAI GPT
- `fs` - Reads and deletes files from the server
- `path` - Handles file paths

---

### Functions in `server/index.js`

#### 1. **splitTranscriptIntoChunks(transcript, maxWords)**

**What it does:**
When a lecture is really long (over 3000 words), we can't send it all to OpenAI at once. This function breaks it into smaller pieces.

**How it works:**
```javascript
// Example:
// Input: "This is a lecture. It's very long. It has many sentences..."
// Output: ["This is a lecture. It's very long.", "It has many sentences..."]
```

1. First, it counts the words in the transcript
2. If it's under 3000 words, just return the whole thing
3. If it's over 3000 words, split it by sentences
4. Group sentences together until we hit 3000 words
5. Start a new chunk and continue
6. Return an array of chunks

**Parameters:**
- `transcript` - The full text to split
- `maxWords` - How many words per chunk (default: 3000)

**Returns:**
An array of text chunks

**Why we need this:**
OpenAI has limits on how much text it can process at once. Also, smaller chunks = faster processing and better results.

---

#### 2. **processTranscriptInChunks(transcript, openaiClient)**

**What it does:**
Takes the chunks from the previous function and sends each one to OpenAI to be organized into notes. Then combines all the notes together.

**How it works:**
1. Call `splitTranscriptIntoChunks()` to break up the transcript
2. Loop through each chunk
3. For each chunk:
   - Send it to OpenAI with instructions to organize it
   - Wait for the response
   - Save the structured notes
4. If there was only 1 chunk, return those notes
5. If there were multiple chunks, combine them with headers like "Part 1", "Part 2", etc.
6. Return the final combined notes

**Parameters:**
- `transcript` - The full text to process
- `openaiClient` - The connection to OpenAI

**Returns:**
Structured notes in markdown format

**The prompt we send to OpenAI:**
```
You are a helpful teaching assistant. Convert this lecture transcript
into well-organized notes with:
- Clear section headers
- Bullet points for main ideas
- Important definitions highlighted
- Examples separated out
```

**Why we need this:**
Long lectures need to be broken down, but we still want one cohesive set of notes at the end.

---

#### 3. **Endpoint: `POST /api/upload-audio`**

**What it does:**
Receives audio files from the frontend and saves them to the server.

**How it works:**
1. Frontend sends audio file
2. Multer middleware intercepts it and saves to `server/uploads/`
3. We check if the file exists
4. Return file information (name, size, path)

**Request format:**
```
POST /api/upload-audio
Content-Type: multipart/form-data
Body: audio file
```

**Response format:**
```json
{
  "message": "Audio uploaded successfully",
  "file": {
    "filename": "recording-1701234567890.webm",
    "size": 245678,
    "path": "/uploads/recording-1701234567890.webm"
  }
}
```

**Error handling:**
- If no file uploaded â†’ 400 error
- If any other error â†’ 500 error

---

#### 4. **Endpoint: `POST /api/transcribe`**

**What it does:**
Takes an audio file and converts it to text using Google Speech-to-Text.

**How it works:**
1. Receive filename from frontend
2. Read the audio file from disk
3. Convert to base64 (Google needs it in this format)
4. Send to Google Speech-to-Text with these settings:
   - Language: English (US)
   - Enhanced model for better accuracy
   - Automatic punctuation
5. Google returns the transcript
6. Delete the audio file (we don't need it anymore)
7. Send transcript back to frontend

**Request format:**
```json
{
  "filename": "recording-1701234567890.webm"
}
```

**Response format:**
```json
{
  "transcript": "Welcome to CS101. Today we'll learn about algorithms...",
  "confidence": 0.95,
  "wordCount": 1523
}
```

**Google API Configuration:**
```javascript
{
  encoding: 'WEBM_OPUS',     // Audio format
  sampleRateHertz: 48000,    // Audio quality
  languageCode: 'en-US',     // English
  model: 'default',          // Use best available model
  enableAutomaticPunctuation: true  // Add periods, commas
}
```

**Error handling:**
- If file doesn't exist â†’ 404 error
- If Google API fails â†’ 500 error with details
- File gets deleted even if there's an error

---

#### 5. **Endpoint: `POST /api/structure-notes`**

**What it does:**
Takes a transcript and uses AI to organize it into structured notes.

**How it works:**
1. Receive transcript from frontend
2. Check if it's empty
3. Count how many words are in it
4. **If under 3000 words:**
   - Send directly to OpenAI
   - Get structured notes back
5. **If over 3000 words:**
   - Call `processTranscriptInChunks()` to handle it
   - This splits it up and processes each part
6. Count words in the output
7. Calculate how long it took
8. Send back the notes + metadata

**Request format:**
```json
{
  "transcript": "Today we're learning about binary trees..."
}
```

**Response format:**
```json
{
  "notes": "# Binary Trees Lecture\n\n## Main Concepts\n- Trees are...",
  "metadata": {
    "inputWords": 1523,
    "outputWords": 892,
    "processingTime": 3.45,
    "model": "Gemini AI"
  }
}
```

**The AI Prompt:**
```javascript
{
  role: "system",
  content: "You are a helpful teaching assistant who creates
           clear, well-organized lecture notes."
},
{
  role: "user",
  content: "Convert this lecture transcript into structured notes..."
}
```

**OpenAI Settings:**
- Model: GPT-3.5-Turbo
- Temperature: 0.3 (more focused, less creative)
- Max tokens: 4096 (about 3000 words max output)

**Error handling:**
- If transcript is empty â†’ 400 error
- If OpenAI fails â†’ 500 error with details

---

## Frontend Files (Client)

### `client/src/App.jsx` - The Container

**What it does:**
This is the "frame" of the app. It doesn't do much actual workâ€”it just sets up the layout and includes the AudioRecorder component.

**Structure:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Gradient Background         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   "Lecture to Notes"      â”‚  â”‚
â”‚  â”‚   Header                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   AudioRecorder           â”‚  â”‚
â”‚  â”‚   Component               â”‚  â”‚
â”‚  â”‚   (All the real work)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Components used:**
- `AudioRecorder` - The main recording component

**Styling:**
- Gradient background (blue to indigo)
- Centered layout
- Max width for readability

**Why it's separate:**
Separation of concerns. App.jsx handles the overall layout, AudioRecorder handles the functionality.

---

### `client/src/components/AudioRecorder.jsx` - The Workhorse

**What it does:**
This is where all the magic happens. It handles recording, uploading, displaying progress, and showing results.

**State Variables (React useState):**

| Variable | What it stores | Example values |
|----------|---------------|----------------|
| `recordingState` | Current recording status | `'idle'`, `'recording'`, `'stopped'` |
| `audioBlob` | The recorded audio file | Binary audio data |
| `error` | Error messages | `"Microphone access denied"` |
| `recordingTime` | How long we've recorded | `65` (seconds) |
| `processStep` | Current processing step | `'uploading'`, `'transcribing'`, `'structuring'`, `'complete'` |
| `isProcessing` | Are we processing? | `true` or `false` |
| `uploadedFile` | File info after upload | `{filename: "rec.webm", size: 123456}` |
| `transcript` | The text transcript | `"Welcome to the lecture..."` |
| `structuredNotes` | The organized notes | `"# Lecture Notes\n## Topic 1..."` |
| `processingMetadata` | Info about processing | `{inputWords: 500, time: 2.3}` |

**Refs (React useRef):**

| Ref | What it stores | Why we need it |
|-----|---------------|----------------|
| `mediaRecorderRef` | The MediaRecorder object | To control recording (start/stop) |
| `audioChunksRef` | Pieces of audio as recorded | Browser gives us audio in chunks |
| `timerRef` | The timer interval | To update recording time every second |

---

### Functions in `AudioRecorder.jsx`

#### Helper Functions

#### 1. **formatTime(seconds)**

**What it does:**
Converts seconds into a nice MM:SS format.

**How it works:**
```javascript
// Example: 125 seconds
// 125 Ã· 60 = 2 minutes (with 5 remaining)
// 125 % 60 = 5 seconds
// Output: "02:05"
```

**Parameters:**
- `seconds` - Number of seconds (e.g., 125)

**Returns:**
- Formatted string (e.g., "02:05")

**Why we need this:**
Makes the timer look professional instead of showing "125 seconds".

---

#### 2. **startTimer()**

**What it does:**
Starts a timer that counts up every second while recording.

**How it works:**
1. Set `recordingTime` to 0
2. Create an interval that runs every 1000ms (1 second)
3. Each time it runs, add 1 to `recordingTime`
4. Store the interval reference so we can stop it later

**Why we need this:**
Shows students how long they've been recording.

---

#### 3. **stopTimer()**

**What it does:**
Stops the recording timer.

**How it works:**
1. Check if timer is running
2. Clear the interval (stops it)
3. Set the reference to null

**Why we need this:**
When recording stops, the timer should stop too.

---

#### Recording Functions

#### 4. **startRecording()**

**What it does:**
Asks for microphone permission and starts recording audio.

**How it works:**
1. Clear any previous errors
2. Ask browser for microphone access with settings:
   ```javascript
   {
     echoCancellation: true,    // Remove echo
     noiseSuppression: true,    // Remove background noise
     sampleRate: 44100,         // CD-quality audio
   }
   ```
3. Create a MediaRecorder object
4. Set up event listeners:
   - `dataavailable` - When audio data comes in, save it
   - `stop` - When recording stops, create the final file
5. Start recording
6. Start the timer
7. Change state to `'recording'`

**Error handling:**
Different error messages for different problems:
- "NotAllowedError" â†’ User denied permission
- "NotFoundError" â†’ No microphone found
- "NotReadableError" â†’ Microphone in use by another app

**The stop event:**
When recording stops:
1. Combine all audio chunks into one Blob
2. Stop the microphone stream (turns off the light on your webcam)
3. Call `processAudio()` to start uploading/transcribing

**Why `async`:**
We need to wait for the user to grant microphone permission.

---

#### 5. **stopRecording()**

**What it does:**
Stops the recording.

**How it works:**
1. Check if we're actually recording
2. Change state to `'stopped'`
3. Tell the MediaRecorder to stop
4. This triggers the 'stop' event we set up earlier

**Why it's simple:**
Most of the work happens in the 'stop' event handler we set up in `startRecording()`.

---

#### API Functions (Talk to Backend)

#### 6. **uploadAudio(blob)**

**What it does:**
Sends the recorded audio file to the server.

**How it works:**
1. Create a FormData object (like a form submission)
2. Add the audio blob with name "audio"
3. Send POST request to `/api/upload-audio`
4. Wait for response
5. If error, throw it
6. Return the file information

**Parameters:**
- `blob` - The audio file (binary data)

**Returns:**
```javascript
{
  filename: "recording-1701234567890.webm",
  size: 245678,
  path: "/uploads/recording-1701234567890.webm"
}
```

**Why FormData:**
That's how browsers send files to serversâ€”same as when you upload a photo on a website.

---

#### 7. **transcribeAudio(filename)**

**What it does:**
Tells the server to transcribe an audio file.

**How it works:**
1. Send POST request to `/api/transcribe`
2. Include the filename in the request body
3. Wait for the server to:
   - Read the file
   - Send it to Google
   - Get the transcript back
4. Return the transcript and metadata

**Parameters:**
- `filename` - Name of the file on the server

**Returns:**
```javascript
{
  transcript: "Welcome to CS101...",
  confidence: 0.95,
  wordCount: 1523
}
```

**Error handling:**
If the server can't transcribe, show the error message to the user.

---

#### 8. **structureNotes(transcriptText)**

**What it does:**
Sends the transcript to the server to be organized by AI.

**How it works:**
1. Send POST request to `/api/structure-notes`
2. Include the transcript in the request body
3. Wait for the server to:
   - Send it to OpenAI
   - Get organized notes back
4. Return the notes and metadata

**Parameters:**
- `transcriptText` - The raw transcript

**Returns:**
```javascript
{
  notes: "# Lecture Notes\n## Introduction\n- Point 1...",
  metadata: {
    inputWords: 1523,
    outputWords: 892,
    processingTime: 3.45,
    model: "Gemini AI"
  }
}
```

**For long transcripts:**
The server automatically handles chunkingâ€”this function doesn't need to know about it.

---

#### 9. **processAudio(blob)**

**What it does:**
This is the main orchestrator. It runs the entire workflow from upload to final notes.

**How it works:**
```
Start
  â†“
Set isProcessing = true
  â†“
Step 1: Upload
  â”œâ”€ Call uploadAudio(blob)
  â”œâ”€ Set processStep = 'uploading'
  â”œâ”€ Wait for result
  â””â”€ Save filename
  â†“
Step 2: Transcribe
  â”œâ”€ Call transcribeAudio(filename)
  â”œâ”€ Set processStep = 'transcribing'
  â”œâ”€ Wait for result
  â”œâ”€ Check if transcript is empty
  â””â”€ Save transcript
  â†“
Step 3: Structure
  â”œâ”€ Call structureNotes(transcript)
  â”œâ”€ Set processStep = 'structuring'
  â”œâ”€ Wait for result
  â””â”€ Save notes and metadata
  â†“
Complete
  â”œâ”€ Set processStep = 'complete'
  â””â”€ Set isProcessing = false
```

**Error handling:**
If anything goes wrong at any step:
1. Log the error to console
2. Show user-friendly message
3. Stop processing
4. Clear the loading state

**Special check:**
If the transcript is empty (no speech detected), show a helpful error:
```
"No speech detected in the audio. Please ensure your microphone
is working and try speaking louder and clearer."
```

**Why async:**
Each step needs to wait for the previous one to finish.

---

#### 10. **startOver()**

**What it does:**
Resets everything so the user can record a new lecture.

**How it works:**
Clear all the state variables:
```javascript
audioBlob = null
recordingTime = 0
error = null
processStep = ''
isProcessing = false
uploadedFile = null
transcript = ''
structuredNotes = ''
processingMetadata = null
recordingState = 'idle'
```

**Why we need this:**
Without this, old data would still be displayed when trying to record again.

---

#### 11. **getProgressSteps()**

**What it does:**
Returns an array of progress steps to show the user where they are in the process.

**How it works:**
Returns an array of objects, each representing a step:
```javascript
[
  {
    id: 'recording',
    label: 'Recording',
    completed: audioBlob !== null,  // True if we have audio
  },
  {
    id: 'uploading',
    label: 'Uploading',
    completed: uploadedFile !== null,  // True if upload done
    active: processStep === 'uploading',  // True if currently uploading
  },
  // ... more steps
]
```

**How the UI uses this:**
- Green checkmark if `completed = true`
- Blue pulsing dot if `active = true`
- Gray number if not started yet

**Why we need this:**
Gives visual feedback so users know the app is working and how much longer to wait.

---

### The JSX Return (UI)

The component returns a big chunk of JSX (HTML-like code). Here's what each section does:

#### Main Recording Card
- Shows the "Audio Recorder" header
- Contains all the controls and status info

#### Progress Indicator
- Shows the 5 steps: Recording â†’ Uploading â†’ Transcribing â†’ Structuring â†’ Complete
- Only visible when we have audio or are processing

#### Status Indicator
- Shows a colored dot (red = recording, yellow = processing, green = ready)
- Shows status text ("Recording...", "Uploading audio...", etc.)
- Shows the recording timer if recording

#### Progress Bar
- Red pulsing bar while recording
- Blue pulsing bar while processing

#### Error Message
- Red box with error icon
- Shows any error messages
- Only visible when there's an error

#### Control Buttons
- **Start Recording** - Big red button (calls `startRecording()`)
- **Stop Recording** - Gray button (calls `stopRecording()`)
- **Start Over** - Blue button (calls `startOver()`)

#### Audio Preview
- Shows a playable audio player with the recording
- Shows file size and duration
- Only visible after recording but before processing

#### Transcript Section
- White card with the full transcript
- Shows word count
- Scrollable if it's long
- Only visible when we have a transcript

#### Structured Notes Section
- White card with organized notes
- Uses ReactMarkdown to render fancy formatting:
  - Headers (# ## ###)
  - Bullet points
  - Bold and italic text
  - Code blocks
- Shows metadata (processing time, word counts, AI model)
- Only visible when we have notes

#### Loading Spinner Overlay
- Full-screen dark overlay
- Spinning blue circle
- Shows current step ("Uploading Audio...", etc.)
- Shows detailed status message
- Only visible while `isProcessing = true`

---

### `client/src/index.css` - Styling

**What it does:**
Defines all the colors, fonts, and animations for the app.

**Key styles:**

1. **Tailwind CSS:**
   - Utility-first CSS framework
   - Classes like `bg-blue-500`, `p-4`, `rounded-lg`
   - We import it at the top

2. **Custom Animations:**
   ```css
   @keyframes fadeIn {
     from { opacity: 0; }
     to { opacity: 1; }
   }
   ```
   - Makes things smoothly appear

3. **Font:**
   - Uses system fonts (looks native on each OS)

4. **Markdown Styling:**
   - Makes headers, bullets, and paragraphs look nice in notes

---

## Configuration Files

### `package.json` (Root)

**What it does:**
Lists all the backend dependencies and scripts.

**Key dependencies:**
```json
{
  "@google-cloud/speech": "^6.8.0",  // Google Speech-to-Text
  "openai": "^4.77.3",               // OpenAI GPT
  "express": "^4.21.2",              // Web server
  "multer": "^1.4.5-lts.1",          // File uploads
  "cors": "^2.8.5"                   // Allow frontend to call backend
}
```

**Scripts:**
- `npm start` - Runs both frontend and backend
- `npm run server` - Runs only backend
- `npm run client` - Runs only frontend

---

### `client/package.json`

**What it does:**
Lists all the frontend dependencies.

**Key dependencies:**
```json
{
  "react": "^19.0.0",              // UI library
  "react-dom": "^19.0.0",          // React for web
  "react-markdown": "^9.0.3",       // Render markdown
  "tailwindcss": "^3.4.17"         // CSS framework
}
```

**Scripts:**
- `npm run dev` - Start development server
- `npm run build` - Build for production

---

### `.env.example`

**What it does:**
Template for environment variables (secrets and API keys).

**Contents:**
```
GOOGLE_APPLICATION_CREDENTIALS=path/to/your/google-credentials.json
OPENAI_API_KEY=your-openai-api-key-here
PORT=3001
```

**How to use:**
1. Copy this to a new file called `.env`
2. Fill in your actual API keys
3. Never commit `.env` to git (it has secrets!)

**Why we need it:**
- Keeps secrets out of the code
- Different developers can use different keys
- Production vs development environments

---

## How Everything Connects

### The Complete Flow (Step by Step)

**1. User Opens the App**
```
Browser requests: http://localhost:5173
  â†“
Vite dev server sends: index.html + bundled React code
  â†“
React loads App.jsx
  â†“
App.jsx loads AudioRecorder.jsx
  â†“
AudioRecorder shows "Ready to Record"
```

**2. User Clicks "Start Recording"**
```
User clicks button
  â†“
startRecording() is called
  â†“
Browser asks: "Allow microphone?"
  â†“
User clicks "Allow"
  â†“
MediaRecorder starts capturing audio
  â†“
Timer starts counting up
  â†“
UI shows red pulsing dot and "Recording..."
  â†“
Audio chunks are saved to audioChunksRef
```

**3. User Clicks "Stop Recording"**
```
User clicks button
  â†“
stopRecording() is called
  â†“
MediaRecorder.stop() is called
  â†“
'stop' event fires
  â†“
All audio chunks combined into one Blob
  â†“
Blob saved to audioBlob state
  â†“
processAudio(blob) is called automatically
```

**4. Processing Begins**
```
processAudio(blob)
  â†“
Step 1: Upload
  â”œâ”€ UI shows "Uploading Audio..."
  â”œâ”€ uploadAudio(blob) called
  â”œâ”€ FormData created with audio file
  â”œâ”€ POST request to http://localhost:3001/api/upload-audio
  â”œâ”€ Server (multer) saves file to server/uploads/
  â”œâ”€ Server responds with filename
  â””â”€ uploadedFile state updated
  â†“
Step 2: Transcribe
  â”œâ”€ UI shows "Transcribing Audio..."
  â”œâ”€ transcribeAudio(filename) called
  â”œâ”€ POST request to http://localhost:3001/api/transcribe
  â”œâ”€ Server reads the audio file
  â”œâ”€ Server sends to Google Speech-to-Text API
  â”œâ”€ Google processes audio (takes 10-30 seconds)
  â”œâ”€ Google returns transcript
  â”œâ”€ Server deletes audio file
  â”œâ”€ Server responds with transcript
  â””â”€ transcript state updated
  â†“
Step 3: Structure
  â”œâ”€ UI shows "Structuring Notes..."
  â”œâ”€ structureNotes(transcript) called
  â”œâ”€ POST request to http://localhost:3001/api/structure-notes
  â”œâ”€ Server counts words in transcript
  â”œâ”€ If > 3000 words:
  â”‚   â”œâ”€ splitTranscriptIntoChunks() called
  â”‚   â”œâ”€ processTranscriptInChunks() called
  â”‚   â”œâ”€ Each chunk sent to OpenAI
  â”‚   â””â”€ Results combined
  â”œâ”€ Else:
  â”‚   â””â”€ Sent directly to OpenAI
  â”œâ”€ OpenAI processes (takes 5-15 seconds)
  â”œâ”€ OpenAI returns structured notes
  â”œâ”€ Server responds with notes + metadata
  â””â”€ structuredNotes and processingMetadata states updated
  â†“
Step 4: Complete
  â”œâ”€ processStep set to 'complete'
  â”œâ”€ isProcessing set to false
  â”œâ”€ Loading spinner disappears
  â”œâ”€ Transcript section appears
  â”œâ”€ Structured Notes section appears
  â””â”€ "Start Over" button appears
```

**5. User Reads Their Notes**
```
Transcript section shows:
  â”œâ”€ Full text transcript
  â””â”€ Word count

Structured Notes section shows:
  â”œâ”€ Markdown-formatted notes with:
  â”‚   â”œâ”€ Headers
  â”‚   â”œâ”€ Bullet points
  â”‚   â”œâ”€ Bold/italic text
  â”‚   â””â”€ Code blocks
  â””â”€ Metadata:
      â”œâ”€ Input word count
      â”œâ”€ Output word count
      â”œâ”€ Processing time
      â””â”€ AI model used
```

**6. User Clicks "Start Over"**
```
startOver() is called
  â†“
All state cleared:
  â”œâ”€ audioBlob = null
  â”œâ”€ transcript = ''
  â”œâ”€ structuredNotes = ''
  â””â”€ ... (everything reset)
  â†“
UI returns to "Ready to Record"
  â†“
User can record a new lecture
```

---

## Key Concepts Explained

### What is a Blob?
A "Blob" (Binary Large Object) is just a file stored in memory. When you record audio, it's saved as a Blob before being uploaded.

### What is FormData?
It's a way to package files for upload, just like when you attach a file to an email.

### What is async/await?
When you call an API or do something that takes time, you use `async/await` to wait for it to finish without freezing the browser.

```javascript
// Without async/await (bad - browser freezes)
const result = transcribeAudio(filename);  // Browser waits here, frozen

// With async/await (good - browser stays responsive)
const result = await transcribeAudio(filename);  // Can cancel, show progress, etc.
```

### What is State?
In React, "state" is data that can change. When state changes, React automatically re-renders the UI.

```javascript
const [count, setCount] = useState(0);

// Later...
setCount(5);  // React automatically updates the UI to show "5"
```

### What is a Ref?
A "ref" is like state, but changing it doesn't re-render the UI. Use it for things that don't need to be displayed.

```javascript
const timerRef = useRef(null);

// We don't need to show the interval ID, just store it
timerRef.current = setInterval(...);
```

### What is Markdown?
A text format that uses symbols for styling:
- `# Header` â†’ Big heading
- `## Smaller Header` â†’ Medium heading
- `- Item` â†’ Bullet point
- `**bold**` â†’ **bold**

---

## Common Questions

**Q: Why do we delete the audio file after transcribing?**
A: To save disk space. Once we have the text, we don't need the audio anymore.

**Q: Why chunk long transcripts?**
A: OpenAI has limits on input size. Also, smaller chunks = better quality notes.

**Q: Why use Google AND OpenAI?**
A: Google is best at speech-to-text. OpenAI is best at organizing text. Use the best tool for each job.

**Q: What if the user's internet is slow?**
A: The app shows progress at each step so they know it's working. Each API call has a timeout.

**Q: Can multiple users use it at once?**
A: Yes! Each upload gets a unique filename (timestamp-based), so files don't conflict.

**Q: What happens if the browser crashes during recording?**
A: The recording is lost (it's only in memory). Nothing is saved until you click "Stop Recording."

**Q: Why use environment variables?**
A: API keys are secret. If you put them in code and push to GitHub, anyone can steal them and use your account.

---

## Debugging Tips

**If recording doesn't work:**
1. Check browser console for errors
2. Make sure you're on HTTPS (or localhost)
3. Check microphone permissions in browser settings

**If upload fails:**
1. Check if server is running (`npm run server`)
2. Check server console for errors
3. Make sure `server/uploads/` folder exists

**If transcription fails:**
1. Check Google Cloud credentials are set
2. Check `.env` file has correct path
3. Check Google Cloud Console for quota limits

**If structuring fails:**
1. Check OpenAI API key is valid
2. Check account has credits
3. Check the transcript isn't empty

**General debugging:**
1. Open browser console (F12)
2. Look for red error messages
3. Check Network tab to see API calls
4. Check server terminal for error logs

---

## Next Steps / Improvements

**Possible enhancements:**
1. Save recordings to database instead of deleting them
2. User accounts and history
3. Download notes as PDF
4. Multiple language support
5. Real-time transcription (as you speak)
6. Edit notes before finalizing
7. Share notes with classmates
8. Summarize key points
9. Generate quiz questions from notes
10. Mobile app version

---

## Summary

**The whole app in one sentence:**
Record audio â†’ Upload to server â†’ Google converts to text â†’ OpenAI organizes into notes â†’ Display results.

**The backend (server/index.js):**
- Receives files
- Talks to Google and OpenAI
- Handles chunking for long lectures
- Returns results

**The frontend (client/src/components/AudioRecorder.jsx):**
- Records audio
- Manages state
- Calls backend APIs
- Shows pretty UI
- Displays results

**Everything else:**
- Configuration files
- Styling
- Dependencies

---

This app is a great example of how modern web apps work:
- **Frontend** handles user interaction
- **Backend** handles complex processing
- **External APIs** provide AI capabilities
- Everything communicates with HTTP requests

Good luck with your presentation! ðŸŽ“
